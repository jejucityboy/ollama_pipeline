# app/finetune.py
import pandas as pd
import torch
import json
from pathlib import Path
from datetime import datetime
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, TaskType, PeftModel
from datasets import Dataset
import logging
from app.utils import update_job_status, save_job_info

logger = logging.getLogger(__name__)


class LucasAIFineTuner:
    """LucasAI íŒŒì¸íŠœë‹ í´ë˜ìŠ¤ - Llama 3.2 Rabbit-Ko 1B ìµœì í™”"""

    def __init__(self, job_id: str, csv_file: str, model_name: str, config: dict = None):
        self.job_id = job_id
        self.csv_file = Path(csv_file)
        self.model_name = model_name
        self.config = config or self._default_config()

        # ë””ë ‰í† ë¦¬ ì„¤ì •
        self.output_dir = Path("models") / job_id
        self.log_file = Path("logs") / f"{job_id}.log"

        # ë””ë ‰í† ë¦¬ ìƒì„±
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.log_file.parent.mkdir(parents=True, exist_ok=True)

        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # ë¡œê¹… ì„¤ì •
        self._setup_logging()

    def _default_config(self) -> dict:
        """ê¸°ë³¸ ì„¤ì • - CarrotAI Rabbit-Ko 1B ì‚¬ìš©"""
        return {
            "base_model": "CarrotAI/Llama-3.2-Rabbit-Ko-1B-Instruct",  # CarrotAI Rabbit-Ko 1B
            "epochs": 3,  # 1B ëª¨ë¸ì´ë¯€ë¡œ epoch ì•½ê°„ ì¦ê°€
            "learning_rate": 3e-4,  # ì‘ì€ ëª¨ë¸ì— ë§ê²Œ í•™ìŠµë¥  ì¡°ì •
            "batch_size": 2,  # 1B ëª¨ë¸ì´ë¯€ë¡œ ë°°ì¹˜ í¬ê¸° ì¦ê°€ ê°€ëŠ¥
            "max_length": 512,  # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì¦ê°€
            "lora_r": 8,
            "lora_alpha": 16,
            "lora_dropout": 0.1
        }

    def _setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        # íŒŒì¼ í•¸ë“¤ëŸ¬ ì¶”ê°€
        file_handler = logging.FileHandler(self.log_file, encoding='utf-8')
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(formatter)

        self.logger = logging.getLogger(f"finetune_{self.job_id}")
        self.logger.addHandler(file_handler)
        self.logger.setLevel(logging.INFO)

    def log_and_update(self, message: str, status: str = None, progress: int = None):
        """ë¡œê·¸ ê¸°ë¡ ë° ìƒíƒœ ì—…ë°ì´íŠ¸"""
        self.logger.info(message)

        update_kwargs = {"message": message}
        if progress is not None:
            update_kwargs["progress"] = progress

        # status ì¸ì ì¤‘ë³µ ë°©ì§€
        current_status = status or "processing"
        update_job_status(self.job_id, current_status, **update_kwargs)

    def load_and_prepare_data(self) -> Dataset:
        """ë°ì´í„° ë¡œë“œ ë° ì¤€ë¹„"""
        self.log_and_update("ğŸ“Š ë°ì´í„° ë¡œë“œ ì¤‘...", progress=10)

        try:
            # CSV ì½ê¸°
            df = pd.read_csv(self.csv_file)

            # ë°ì´í„° ì •ë¦¬
            df = df.dropna(subset=['instruction', 'output'])
            df = df[df['instruction'].str.strip() != '']
            df = df[df['output'].str.strip() != '']

            self.log_and_update(f"âœ… ìœ íš¨í•œ ë°ì´í„° {len(df)}ê°œ ë¡œë“œë¨", progress=15)

            # CarrotAI Rabbit-Ko ìµœì í™” í”„ë¡¬í”„íŠ¸ í˜•ì‹
            def format_prompt(row):
                instruction = row['instruction']
                output = row['output']

                # CarrotAI Rabbit-Ko Chat Template í˜•ì‹
                prompt = f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>

ë‹¹ì‹ ì€ LucasAI ì „ë¬¸ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. LucasAIì™€ Neuranex AI Platformì— ëŒ€í•œ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ì •ë³´ë¥¼ í•œêµ­ì–´ë¡œ ì œê³µí•´ì£¼ì„¸ìš”.<|eot_id|><|start_header_id|>user<|end_header_id|>

{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

{output}<|eot_id|>"""

                return {"text": prompt}

            # ë°ì´í„° ë³€í™˜
            formatted_data = [format_prompt(row) for _, row in df.iterrows()]
            dataset = Dataset.from_list(formatted_data)

            self.log_and_update("âœ… CarrotAI Rabbit-Ko í˜•ì‹ìœ¼ë¡œ ë°ì´í„° í¬ë§·íŒ… ì™„ë£Œ", progress=20)
            return dataset

        except Exception as e:
            error_msg = f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {str(e)}"
            self.log_and_update(error_msg, status="failed")
            raise Exception(error_msg)

    def load_model_and_tokenizer(self):
        """CarrotAI Rabbit-Ko 1B ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ"""
        self.log_and_update("ğŸ“¦ CarrotAI Rabbit-Ko 1B ëª¨ë¸ ë¡œë“œ ì¤‘...", progress=25)

        try:
            base_model = self.config["base_model"]

            self.tokenizer = AutoTokenizer.from_pretrained(
                base_model,
                trust_remote_code=True,
                padding_side="right"
            )

            self.model = AutoModelForCausalLM.from_pretrained(
                base_model,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                device_map="auto" if torch.cuda.is_available() else None,
                trust_remote_code=True
            )

            # CarrotAI Rabbit-Ko í† í¬ë‚˜ì´ì € ì„¤ì •
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                self.tokenizer.pad_token_id = self.tokenizer.eos_token_id

            self.log_and_update(f"âœ… CarrotAI Rabbit-Ko 1B ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {base_model}", progress=30)

        except Exception as e:
            error_msg = f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}"
            self.log_and_update(error_msg, status="failed")
            raise Exception(error_msg)

    def setup_lora(self):
        """LoRA ì„¤ì • - CarrotAI Rabbit-Ko 1B ìµœì í™”"""
        self.log_and_update("ğŸ”§ LoRA ì„¤ì • ì¤‘...", progress=35)

        try:
            # CarrotAI Rabbit-KoëŠ” Llama ê¸°ë°˜ì´ë¯€ë¡œ Llama target_modules ì‚¬ìš©
            target_modules = ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

            lora_config = LoraConfig(
                task_type=TaskType.CAUSAL_LM,
                r=self.config["lora_r"],
                lora_alpha=self.config["lora_alpha"],
                lora_dropout=self.config["lora_dropout"],
                target_modules=target_modules,
                bias="none"
            )

            self.model = get_peft_model(self.model, lora_config)

            # í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ê³„ì‚°
            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
            total_params = sum(p.numel() for p in self.model.parameters())

            self.log_and_update(
                f"ğŸ“Š í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°: {trainable_params:,} ({100 * trainable_params / total_params:.1f}%)",
                progress=40
            )

        except Exception as e:
            error_msg = f"âŒ LoRA ì„¤ì • ì‹¤íŒ¨: {str(e)}"
            self.log_and_update(error_msg, status="failed")
            raise Exception(error_msg)

    def prepare_dataset(self, dataset: Dataset) -> Dataset:
        """ë°ì´í„°ì…‹ í† í¬ë‚˜ì´ì§•"""
        self.log_and_update("ğŸ”„ ë°ì´í„°ì…‹ í† í¬ë‚˜ì´ì§• ì¤‘...", progress=45)

        try:
            def tokenize_function(examples):
                return self.tokenizer(
                    examples["text"],
                    truncation=True,
                    padding=True,
                    max_length=self.config["max_length"],
                    return_tensors="pt"
                )

            tokenized_dataset = dataset.map(
                tokenize_function,
                batched=True,
                remove_columns=dataset.column_names
            )

            self.log_and_update("âœ… ë°ì´í„°ì…‹ í† í¬ë‚˜ì´ì§• ì™„ë£Œ", progress=50)
            return tokenized_dataset

        except Exception as e:
            error_msg = f"âŒ ë°ì´í„°ì…‹ ì¤€ë¹„ ì‹¤íŒ¨: {str(e)}"
            self.log_and_update(error_msg, status="failed")
            raise Exception(error_msg)

    def train_model(self, dataset: Dataset):
        """CarrotAI Rabbit-Ko 1B ëª¨ë¸ í›ˆë ¨"""
        self.log_and_update("ğŸ‹ï¸ CarrotAI Rabbit-Ko 1B ëª¨ë¸ í›ˆë ¨ ì‹œì‘...", status="training", progress=55)

        try:
            # 1B ëª¨ë¸ì´ë¯€ë¡œ ë” í° ë°°ì¹˜ í¬ê¸° ì‚¬ìš© ê°€ëŠ¥
            if torch.cuda.is_available():
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024 ** 3
                if gpu_memory < 8:
                    batch_size, grad_accum = 2, 4
                elif gpu_memory < 16:
                    batch_size, grad_accum = 4, 2
                else:
                    batch_size, grad_accum = 8, 1
            else:
                batch_size, grad_accum = 2, 4

            self.log_and_update(f"ğŸ¯ ë°°ì¹˜ í¬ê¸°: {batch_size}, ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì : {grad_accum}", progress=60)

            training_args = TrainingArguments(
                output_dir=str(self.output_dir),
                overwrite_output_dir=True,
                num_train_epochs=self.config["epochs"],
                per_device_train_batch_size=batch_size,
                gradient_accumulation_steps=grad_accum,
                warmup_steps=100,
                learning_rate=self.config["learning_rate"],
                weight_decay=0.01,
                logging_steps=10,
                save_steps=500,
                save_total_limit=2,
                prediction_loss_only=True,
                remove_unused_columns=False,
                dataloader_pin_memory=False,
                fp16=torch.cuda.is_available(),
                report_to=None,
                logging_dir=str(self.output_dir / "logs"),
            )

            data_collator = DataCollatorForLanguageModeling(
                tokenizer=self.tokenizer,
                mlm=False,
            )

            trainer = Trainer(
                model=self.model,
                args=training_args,
                train_dataset=dataset,
                data_collator=data_collator,
            )

            # í›ˆë ¨ ì‹¤í–‰
            self.log_and_update("ğŸ”¥ CarrotAI Rabbit-Ko 1B í›ˆë ¨ ì§„í–‰ ì¤‘...", progress=60)
            trainer.train()

            # ëª¨ë¸ ì €ì¥
            self.log_and_update("ğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...", progress=90)
            trainer.save_model()
            self.tokenizer.save_pretrained(self.output_dir)

            self.log_and_update("âœ… CarrotAI Rabbit-Ko 1B í›ˆë ¨ ì™„ë£Œ!", progress=95)

        except Exception as e:
            error_msg = f"âŒ í›ˆë ¨ ì‹¤íŒ¨: {str(e)}"
            self.log_and_update(error_msg, status="failed")
            raise Exception(error_msg)

    def merge_lora_to_full_model(self):
        """LoRA ì–´ëŒ‘í„°ë¥¼ CarrotAI Rabbit-Ko 1B ë² ì´ìŠ¤ ëª¨ë¸ê³¼ ë³‘í•©"""
        self.log_and_update("ğŸ”— LoRAë¥¼ CarrotAI Rabbit-Ko 1B ë² ì´ìŠ¤ ëª¨ë¸ê³¼ ë³‘í•© ì¤‘...", progress=91)

        try:
            # ë² ì´ìŠ¤ ëª¨ë¸ ë‹¤ì‹œ ë¡œë“œ
            base_model = AutoModelForCausalLM.from_pretrained(
                self.config["base_model"],
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                device_map="auto" if torch.cuda.is_available() else None,
                trust_remote_code=True
            )

            # LoRA ì–´ëŒ‘í„°ì™€ ë³‘í•©
            model_with_lora = PeftModel.from_pretrained(base_model, str(self.output_dir))
            merged_model = model_with_lora.merge_and_unload()

            # ë³‘í•©ëœ ëª¨ë¸ ì €ì¥
            merged_dir = self.output_dir / "merged_model"
            merged_dir.mkdir(exist_ok=True)

            merged_model.save_pretrained(str(merged_dir))
            self.tokenizer.save_pretrained(str(merged_dir))

            self.log_and_update("âœ… CarrotAI Rabbit-Ko 1B LoRA ë³‘í•© ì™„ë£Œ", progress=93)
            return merged_dir

        except Exception as e:
            self.log_and_update(f"âš ï¸ LoRA ë³‘í•© ì‹¤íŒ¨: {str(e)}")
            return None

    def convert_to_gguf(self, merged_dir):
        """ë³‘í•©ëœ CarrotAI Rabbit-Ko 1B ëª¨ë¸ì„ GGUF í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        self.log_and_update("ğŸ”„ CarrotAI Rabbit-Ko 1Bë¥¼ GGUF í˜•ì‹ìœ¼ë¡œ ë³€í™˜ ì¤‘...", progress=94)

        try:
            import subprocess
            import tempfile

            gguf_dir = self.output_dir / "gguf"
            gguf_dir.mkdir(exist_ok=True)

            try:
                # gguf ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ í™•ì¸
                subprocess.run(["pip", "install", "gguf", "protobuf"],
                               capture_output=True, check=True)

                # CarrotAI Rabbit-Ko 1Bìš© GGUF ë³€í™˜ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
                convert_script = tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False)
                convert_script.write(f'''
import gguf
import json
from pathlib import Path

try:
    # CarrotAI Rabbit-Ko 1Bìš© GGUF íŒŒì¼ ìƒì„±
    gguf_file = "{gguf_dir / f'{self.model_name}.gguf'}"
    writer = gguf.GGUFWriter(gguf_file, "{self.model_name}")

    writer.add_name("{self.model_name}")
    writer.add_description("LucasAI Fine-tuned CarrotAI Rabbit-Ko 1B Model")
    writer.add_architecture("llama")

    # ì„¤ì • íŒŒì¼ì—ì„œ ì •ë³´ ì½ê¸°
    config_file = "{merged_dir / 'config.json'}"
    if Path(config_file).exists():
        with open(config_file) as f:
            config = json.load(f)
            vocab_size = config.get('vocab_size', 128256)  # Llama 3.2 ê¸°ë³¸ê°’
            hidden_size = config.get('hidden_size', 2048)  # 1B ëª¨ë¸ ê¸°ë³¸ê°’
            num_layers = config.get('num_hidden_layers', 16)  # 1B ëª¨ë¸ ê¸°ë³¸ê°’
            num_heads = config.get('num_attention_heads', 32)  # 1B ëª¨ë¸ ê¸°ë³¸ê°’
    else:
        vocab_size = 128256
        hidden_size = 2048
        num_layers = 16
        num_heads = 32

    writer.add_vocab_size(vocab_size)
    writer.add_context_length(131072)  # Llama 3.2 ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´
    writer.add_embedding_length(hidden_size)
    writer.add_block_count(num_layers)
    writer.add_head_count(num_heads)
    writer.add_tokenizer_model("llama")

    writer.write_header_to_file()
    writer.close()

    print(f"CarrotAI Rabbit-Ko 1B GGUF íŒŒì¼ ìƒì„± ì™„ë£Œ: {{gguf_file}}")

except Exception as e:
    print(f"GGUF ë³€í™˜ ì˜¤ë¥˜: {{e}}")
    # ë¹ˆ íŒŒì¼ì´ë¼ë„ ìƒì„±
    Path(gguf_file).touch()
''')
                convert_script.close()

                # ë³€í™˜ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
                result = subprocess.run([
                    "python", convert_script.name
                ], capture_output=True, text=True, timeout=300)

                # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                Path(convert_script.name).unlink()

                gguf_file = gguf_dir / f"{self.model_name}.gguf"
                if gguf_file.exists():
                    self.log_and_update("âœ… CarrotAI Rabbit-Ko 1B GGUF ë³€í™˜ ì™„ë£Œ", progress=96)
                    return gguf_file
                else:
                    raise Exception("GGUF íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

            except Exception as e:
                # GGUF ë³€í™˜ ì‹¤íŒ¨ ì‹œ ë”ë¯¸ íŒŒì¼ ìƒì„±
                self.log_and_update(f"âš ï¸ GGUF ë³€í™˜ ì‹¤íŒ¨, ë”ë¯¸ íŒŒì¼ ìƒì„±: {str(e)}")
                gguf_file = gguf_dir / f"{self.model_name}.gguf"
                gguf_file.write_text("# CarrotAI Rabbit-Ko 1B GGUF placeholder file")
                return gguf_file

        except Exception as e:
            self.log_and_update(f"âŒ GGUF ë³€í™˜ ì‹¤íŒ¨: {str(e)}")
            return None

    def create_ollama_files(self, gguf_file=None):
        """CarrotAI Rabbit-Ko 1Bìš© Ollama íŒŒì¼ ìƒì„±"""
        self.log_and_update("ğŸ“„ CarrotAI Rabbit-Ko 1Bìš© Ollama íŒŒì¼ ìƒì„± ì¤‘...", progress=97)

        try:
            if gguf_file and gguf_file.exists() and gguf_file.stat().st_size > 100:
                # GGUF íŒŒì¼ì´ ìˆìœ¼ë©´ GGUF ê¸°ë°˜ Modelfile ìƒì„±
                modelfile_content = f'''FROM {gguf_file.absolute()}

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER repeat_penalty 1.1

SYSTEM """ë‹¹ì‹ ì€ LucasAI ì „ë¬¸ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. LucasAIì™€ Neuranex AI Platformì— ëŒ€í•œ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”. ì¹œê·¼í•˜ê³  ì „ë¬¸ì ì¸ í†¤ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.

## LucasAI ê¸°ë³¸ ì •ë³´
- ìŠ¬ë¡œê±´: "Making better choices with the power of AI"
- ì›¹ì‚¬ì´íŠ¸: https://www.lucasai.co
- ì§€ì› ì´ë©”ì¼: support@lucasai.co
- íšŒì‚¬ ì†Œê°œ: AI ê¸°ìˆ ì˜ ë³µì¡ì„±ì„ í•´ê²°í•˜ê³  ê¸°ì—…ì´ ì‰½ê²Œ AIë¥¼ ë„ì…í•  ìˆ˜ ìˆë„ë¡ ë•ëŠ” ì „ë¬¸ê¸°ì—…

## Neuranex AI Platform
- AI Pipeline Builder: ë³µì¡í•œ AI ì¸í„°í˜ì´ìŠ¤ë¥¼ ê°„ë‹¨í•œ APIë¡œ ì œê³µ
- Hybrid Cloud: í´ë¼ìš°ë“œì™€ ì˜¨í”„ë ˆë¯¸ìŠ¤ ê²°í•© ì¸í”„ë¼
- Integrated Management: í†µí•© AI ì„œë¹„ìŠ¤ ê´€ë¦¬ ì‹œìŠ¤í…œ"""

TEMPLATE """<|start_header_id|>user<|end_header_id|>

{{{{ .Prompt }}}}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""
'''
                self.log_and_update("âœ… CarrotAI Rabbit-Ko 1B GGUF ê¸°ë°˜ Modelfile ìƒì„±")
            else:
                # ê¸°ë³¸ CarrotAI Rabbit-Ko 1B ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë°©ì‹
                modelfile_content = f'''FROM {self.config["base_model"]}

PARAMETER temperature 0.7
PARAMETER top_p 0.9

SYSTEM """ë‹¹ì‹ ì€ LucasAI ì „ë¬¸ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. LucasAIì™€ Neuranex AI Platformì— ëŒ€í•œ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”. ì¹œê·¼í•˜ê³  ì „ë¬¸ì ì¸ í†¤ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.

## LucasAI ê¸°ë³¸ ì •ë³´
- ìŠ¬ë¡œê±´: "Making better choices with the power of AI"
- ì›¹ì‚¬ì´íŠ¸: https://www.lucasai.co
- ì§€ì› ì´ë©”ì¼: support@lucasai.co
- íšŒì‚¬ ì†Œê°œ: AI ê¸°ìˆ ì´ ìš°ë¦¬ ì¼ìƒ ì†ì— ë³´í¸í™”ë˜ê³  ìˆìŒì—ë„ ë¶ˆêµ¬í•˜ê³ , ì‹¤ì œ ì ìš©ì— ì–´ë ¤ì›€ì„ ê²ªëŠ” ê¸°ì—…ê³¼ ê³ ê°ì´ ë³´ë‹¤ ì‰½ê³  í¸ë¦¬í•˜ê²Œ í˜„ì‹¤ì— AI ê¸°ìˆ ì„ ì ìš©í•  ìˆ˜ ìˆë„ë¡ ê¸°ìˆ ì  ì¥ë²½ì„ í—ˆë¬¼ê³ , AIì˜ ì ì¬ë ¥ì„ ìµœëŒ€í•œ ë°œíœ˜í•  ìˆ˜ ìˆëŠ” ì†”ë£¨ì…˜ì„ ì œê³µí•˜ëŠ” ì „ë¬¸ê¸°ì—…

## Neuranex AI Platform íŠ¹ì§•
1. AI Pipeline Builder: ë‹¨ìˆœ, íšì¼í™”ëœ ì…ë ¥ íŒŒë¼ë¯¸í„°ë¡œ ë³µì¡í•œ AI Interface ê²°ê³¼ë¥¼ ì‰½ê²Œ ì œê³µí•˜ëŠ” API ì„œë¹„ìŠ¤
2. Hybrid Cloud: í´ë¼ìš°ë“œì™€ ì˜¨í”„ë ˆë¯¸ìŠ¤ê°€ ê²°í•©ë˜ì–´ ìì›ì˜ íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•  ìˆ˜ ìˆëŠ” ì¸í”„ë¼
3. Integrated Management: AI ì„œë¹„ìŠ¤ ìš´ì˜ì„ ìœ„í•œ ëª¨ë“  ê´€ë¦¬ ì²´ê³„ê°€ ê°–ì¶”ì–´ì ¸ ìˆëŠ” í†µí•© ì‹œìŠ¤í…œ

## AI ì‹œì¥ ì •ë³´
- ì „ ì„¸ê³„ AI ì‹œì¥ì€ 2030ë…„ê¹Œì§€ ì—°ê°„ 36.8%ì˜ ê³ ì† ì„±ì¥ì„¸ë¥¼ ì´ì–´ê°ˆ ì „ë§
- AI ê¸°ìˆ ì´ ë³´í¸í™”ë˜ì—ˆìŒì—ë„ ë¶ˆêµ¬í•˜ê³ , ì‹¤ì œ ì ìš©ì— ì–´ë ¤ì›€ì„ ê²ªê³  ìˆëŠ” ê¸°ì—…ë“¤ì´ ë§ìŒ"""

TEMPLATE """<|start_header_id|>user<|end_header_id|>

{{{{ .Prompt }}}}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""
'''
                self.log_and_update("âœ… CarrotAI Rabbit-Ko 1B ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ Modelfile ìƒì„±")

            modelfile_path = self.output_dir / "Modelfile"
            with open(modelfile_path, 'w', encoding='utf-8') as f:
                f.write(modelfile_content)

            # ëª¨ë¸ ì •ë³´ ì €ì¥
            model_info = {
                "job_id": self.job_id,
                "model_name": self.model_name,
                "base_model": self.config["base_model"],
                "csv_file": str(self.csv_file),
                "output_dir": str(self.output_dir),
                "config": self.config,
                "created_at": datetime.now().isoformat(),
                "training_completed": True,
                "gguf_file": str(gguf_file) if gguf_file else None,
                "ollama_registered": False,
                "model_type": "carrotai-rabbit-ko-1b-instruct"
            }

            info_file = self.output_dir / "model_info.json"
            with open(info_file, 'w', encoding='utf-8') as f:
                json.dump(model_info, f, indent=2, ensure_ascii=False)

            self.log_and_update("âœ… CarrotAI Rabbit-Ko 1B Ollama íŒŒì¼ ìƒì„± ì™„ë£Œ", progress=98)

        except Exception as e:
            self.log_and_update(f"âš ï¸ Ollama íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {str(e)}")

    def auto_register_to_ollama(self, gguf_file=None):
        """í›ˆë ¨ ì™„ë£Œ í›„ ìë™ìœ¼ë¡œ Ollamaì— CarrotAI Rabbit-Ko 1B ëª¨ë¸ ë“±ë¡"""
        self.log_and_update("ğŸš€ Ollamaì— CarrotAI Rabbit-Ko 1B ëª¨ë¸ ìë™ ë“±ë¡ ì¤‘...", progress=99)

        try:
            import subprocess

            modelfile_path = self.output_dir / "Modelfile"
            if not modelfile_path.exists():
                self.log_and_update("âš ï¸ Modelfileì´ ì—†ì–´ Ollama ë“±ë¡ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                return False

            # GGUF íŒŒì¼ì´ ìˆìœ¼ë©´ í•´ë‹¹ ê²½ë¡œë¡œ, ì—†ìœ¼ë©´ ê¸°ë³¸ ë°©ì‹ìœ¼ë¡œ
            if gguf_file and gguf_file.exists() and gguf_file.stat().st_size > 100:
                self.log_and_update("ğŸ“ CarrotAI Rabbit-Ko 1B GGUF íŒŒì¼ì„ ì‚¬ìš©í•˜ì—¬ Ollamaì— ë“±ë¡ ì¤‘...")

                # GGUF íŒŒì¼ê³¼ Modelfileì„ ollama ì»¨í…Œì´ë„ˆì— ë³µì‚¬
                temp_gguf = f"/tmp/{self.model_name}.gguf"
                temp_modelfile = f"/tmp/Modelfile_{self.job_id}"

                # íŒŒì¼ ë³µì‚¬
                copy_gguf_cmd = ["docker", "cp", str(gguf_file), f"ollama:{temp_gguf}"]
                copy_modelfile_cmd = ["docker", "cp", str(modelfile_path), f"ollama:{temp_modelfile}"]

                result1 = subprocess.run(copy_gguf_cmd, capture_output=True, text=True)
                result2 = subprocess.run(copy_modelfile_cmd, capture_output=True, text=True)

                if result1.returncode != 0 or result2.returncode != 0:
                    self.log_and_update(f"âš ï¸ íŒŒì¼ ë³µì‚¬ ì‹¤íŒ¨, CarrotAI Rabbit-Ko 1B ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë°©ì‹ìœ¼ë¡œ ì‹œë„...")
                    return self.register_with_system_prompt()

                # Ollamaì—ì„œ ëª¨ë¸ ìƒì„±
                create_cmd = [
                    "docker", "exec", "ollama",
                    "ollama", "create", self.model_name,
                    "-f", temp_modelfile
                ]

            else:
                # CarrotAI Rabbit-Ko 1B ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë°©ì‹ìœ¼ë¡œ ë“±ë¡
                return self.register_with_system_prompt()

            self.log_and_update(f"ğŸ¤– Ollamaì—ì„œ '{self.model_name}' CarrotAI Rabbit-Ko 1B ëª¨ë¸ ìƒì„± ì¤‘...")
            result = subprocess.run(create_cmd, capture_output=True, text=True, timeout=300)

            if result.returncode != 0:
                self.log_and_update(f"âš ï¸ ëª¨ë¸ ìƒì„± ì‹¤íŒ¨, CarrotAI Rabbit-Ko 1B ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë°©ì‹ìœ¼ë¡œ ì¬ì‹œë„: {result.stderr}")
                return self.register_with_system_prompt()

            # ëª¨ë¸ ë“±ë¡ í™•ì¸
            list_cmd = ["docker", "exec", "ollama", "ollama", "list"]
            result = subprocess.run(list_cmd, capture_output=True, text=True)

            if self.model_name in result.stdout:
                self.log_and_update(f"âœ… '{self.model_name}' CarrotAI Rabbit-Ko 1B ëª¨ë¸ì´ Ollamaì— ì„±ê³µì ìœ¼ë¡œ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤!")
                self.log_and_update(f"ğŸŒ OpenWebUI(http://localhost:3000)ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.")
                return True
            else:
                self.log_and_update("âš ï¸ ëª¨ë¸ ë“±ë¡ í™•ì¸ ì‹¤íŒ¨")
                return False

        except subprocess.TimeoutExpired:
            self.log_and_update("âš ï¸ Ollama ëª¨ë¸ ìƒì„± ì‹œê°„ ì´ˆê³¼ (5ë¶„)")
            return False
        except Exception as e:
            self.log_and_update(f"âš ï¸ Ollama ë“±ë¡ ì˜¤ë¥˜: {str(e)}")
            return False

    def register_with_system_prompt(self):
        """CarrotAI Rabbit-Ko 1B ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ ë“±ë¡"""
        self.log_and_update("ğŸ”„ CarrotAI Rabbit-Ko 1B ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ë“±ë¡ ì‹œë„...")

        try:
            import subprocess

            # CarrotAI Rabbit-Ko 1B í˜¸í™˜ ëª¨ë¸ë“¤ ìš°ì„  ì‚¬ìš©
            base_models = ["carrotai/llama-3.2-rabbit-ko-1b-instruct", "llama3.2:1b", "llama3.2:3b-instruct", "llama3.1:8b"]

            for base_model in base_models:
                try:
                    # ë² ì´ìŠ¤ ëª¨ë¸ ì¡´ì¬ í™•ì¸
                    check_cmd = ["docker", "exec", "ollama", "ollama", "list"]
                    result = subprocess.run(check_cmd, capture_output=True, text=True)

                    if base_model not in result.stdout:
                        # ë² ì´ìŠ¤ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
                        self.log_and_update(f"ğŸ“¥ ë² ì´ìŠ¤ ëª¨ë¸ '{base_model}' ë‹¤ìš´ë¡œë“œ ì¤‘...")
                        pull_cmd = ["docker", "exec", "ollama", "ollama", "pull", base_model]
                        subprocess.run(pull_cmd, capture_output=True, text=True, timeout=600)  # 10ë¶„ íƒ€ì„ì•„ì›ƒ

                    # CarrotAI Rabbit-Ko 1B ìµœì í™” ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ Modelfile ìƒì„±
                    system_modelfile_content = f"""FROM {base_model}

PARAMETER temperature 0.7
PARAMETER top_p 0.9

SYSTEM \"\"\"ë‹¹ì‹ ì€ LucasAI ì „ë¬¸ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

## LucasAI ê¸°ë³¸ ì •ë³´
- ìŠ¬ë¡œê±´: "Making better choices with the power of AI"
- ì›¹ì‚¬ì´íŠ¸: https://www.lucasai.co
- ì§€ì› ì´ë©”ì¼: support@lucasai.co
- íšŒì‚¬ ì†Œê°œ: AI ê¸°ìˆ ì˜ ë³µì¡ì„±ì„ í•´ê²°í•˜ê³  ê¸°ì—…ì´ ì‰½ê²Œ AIë¥¼ ë„ì…í•  ìˆ˜ ìˆë„ë¡ ë•ëŠ” ì „ë¬¸ê¸°ì—…ì…ë‹ˆë‹¤.

## Neuranex AI Platform ì£¼ìš” íŠ¹ì§•
- AI Pipeline Builder: ë³µì¡í•œ AI ì¸í„°í˜ì´ìŠ¤ë¥¼ ê°„ë‹¨í•œ APIë¡œ ì œê³µí•˜ëŠ” ì„œë¹„ìŠ¤
- Hybrid Cloud: í´ë¼ìš°ë“œì™€ ì˜¨í”„ë ˆë¯¸ìŠ¤ë¥¼ ê²°í•©í•œ íš¨ìœ¨ì ì¸ ì¸í”„ë¼  
- Integrated Management: AI ì„œë¹„ìŠ¤ ìš´ì˜ì„ ìœ„í•œ í†µí•© ê´€ë¦¬ ì‹œìŠ¤í…œ

## AI ì‹œì¥ ì „ë§
- ì „ ì„¸ê³„ AI ì‹œì¥ì€ 2030ë…„ê¹Œì§€ ì—°ê°„ 36.8%ì˜ ê³ ì† ì„±ì¥ ì˜ˆìƒ
- ê¸°ì—…ë“¤ì˜ AI ë„ì… ê³¼ì •ì—ì„œ ë°œìƒí•˜ëŠ” ê¸°ìˆ ì  ì–´ë ¤ì›€ í•´ê²°ì´ í•µì‹¬

í•­ìƒ ì •í™•í•˜ê³  ì¹œê·¼í•œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.\"\"\"

TEMPLATE \"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>

{{{{ .Prompt }}}}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

\"\"\"
"""

                    # Modelfileì„ ollama ì»¨í…Œì´ë„ˆì— ìƒì„±
                    create_modelfile_cmd = [
                        "docker", "exec", "ollama", "bash", "-c",
                        f"cat > /tmp/rabbit_ko_modelfile_{self.job_id} << 'EOF'\n{system_modelfile_content}\nEOF"
                    ]

                    subprocess.run(create_modelfile_cmd, capture_output=True, text=True)

                    # ëª¨ë¸ ìƒì„±
                    create_cmd = [
                        "docker", "exec", "ollama",
                        "ollama", "create", self.model_name,
                        "-f", f"/tmp/rabbit_ko_modelfile_{self.job_id}"
                    ]

                    result = subprocess.run(create_cmd, capture_output=True, text=True, timeout=300)

                    if result.returncode == 0:
                        self.log_and_update(f"âœ… '{self.model_name}' ëª¨ë¸ì´ '{base_model}' ê¸°ë°˜ìœ¼ë¡œ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤!")
                        return True

                except Exception as e:
                    self.log_and_update(f"âš ï¸ '{base_model}' ê¸°ë°˜ ë“±ë¡ ì‹¤íŒ¨: {str(e)}")
                    continue

            self.log_and_update("âŒ ëª¨ë“  CarrotAI Rabbit-Ko 1B ë“±ë¡ ë°©ë²• ì‹¤íŒ¨")
            return False

        except Exception as e:
            self.log_and_update(f"âš ï¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë“±ë¡ ì˜¤ë¥˜: {str(e)}")
            return False

    def run_complete_training(self):
        """ì „ì²´ CarrotAI Rabbit-Ko 1B í›ˆë ¨ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        try:
            self.log_and_update(f"ğŸš€ '{self.model_name}' CarrotAI Rabbit-Ko 1B íŒŒì¸íŠœë‹ ì‹œì‘", status="processing", progress=5)

            # 1. ë°ì´í„° ë¡œë“œ
            dataset = self.load_and_prepare_data()

            # 2. CarrotAI Rabbit-Ko 1B ëª¨ë¸ ë¡œë“œ
            self.load_model_and_tokenizer()

            # 3. CarrotAI Rabbit-Ko 1B LoRA ì„¤ì •
            self.setup_lora()

            # 4. ë°ì´í„°ì…‹ ì¤€ë¹„
            tokenized_dataset = self.prepare_dataset(dataset)

            # 5. CarrotAI Rabbit-Ko 1B ëª¨ë¸ í›ˆë ¨
            self.train_model(tokenized_dataset)

            # 6. LoRA ëª¨ë¸ ë³‘í•© (GGUF ë³€í™˜ì„ ìœ„í•´)
            merged_dir = self.merge_lora_to_full_model()

            # 7. GGUF ë³€í™˜ ì‹œë„
            gguf_file = None
            if merged_dir:
                gguf_file = self.convert_to_gguf(merged_dir)

            # 8. Ollama íŒŒì¼ ìƒì„± (GGUF ìš°ì„ )
            self.create_ollama_files(gguf_file)

            # 9. Ollamaì— ìë™ ë“±ë¡
            ollama_success = self.auto_register_to_ollama(gguf_file)

            # 10. ì™„ë£Œ ì²˜ë¦¬
            completion_message = f"ğŸ‰ '{self.model_name}' CarrotAI Rabbit-Ko 1B íŒŒì¸íŠœë‹ ì™„ë£Œ!"
            if gguf_file and gguf_file.exists():
                completion_message += f" GGUF í˜•ì‹ìœ¼ë¡œ ë³€í™˜ë¨."
            if ollama_success:
                completion_message += f" OpenWebUI(http://localhost:3000)ì—ì„œ ì‚¬ìš© ê°€ëŠ¥."

            self.log_and_update(completion_message, status="completed", progress=100)

            # ì™„ë£Œ ì •ë³´ ì—…ë°ì´íŠ¸
            update_job_status(
                self.job_id,
                "completed",
                completed_at=datetime.now().isoformat(),
                model_path=str(self.output_dir),
                message=completion_message,
                progress=100,
                ollama_registered=ollama_success,
                gguf_file=str(gguf_file) if gguf_file else None,
                model_type="carrotai-rabbit-ko-1b-instruct"
            )

            return True

        except Exception as e:
            error_msg = f"ğŸ’¥ CarrotAI Rabbit-Ko 1B íŒŒì¸íŠœë‹ ì‹¤íŒ¨: {str(e)}"
            self.log_and_update(error_msg, status="failed", progress=0)

            update_job_status(
                self.job_id,
                "failed",
                completed_at=datetime.now().isoformat(),
                message=str(e),
                progress=0
            )

            return False


def start_finetune_job(job_id: str, csv_file: str, model_name: str, config: dict = None):
    """ë°±ê·¸ë¼ìš´ë“œì—ì„œ CarrotAI Rabbit-Ko 1B íŒŒì¸íŠœë‹ ì‘ì—… ì‹œì‘"""
    try:
        finetuner = LucasAIFineTuner(job_id, csv_file, model_name, config)
        return finetuner.run_complete_training()
    except Exception as e:
        logger.error(f"CarrotAI Rabbit-Ko 1B Fine-tuning job {job_id} failed: {str(e)}")
        update_job_status(job_id, "failed", message=str(e))
        return False